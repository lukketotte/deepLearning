{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net:\n",
    "    \"\"\"\n",
    "    https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Forwad-Back-Propagation.ipynb\n",
    "    \n",
    "    link_function: specifies the link function. Possible values are\n",
    "                   \"sigmoid\" and \"softmax\"\n",
    "    \n",
    "    batch_size: int. Specify the sample size of each bin. Uses np.array_split to avoid\n",
    "                exception due to not even split possible.  Set to None to avoid it.\n",
    "                   \n",
    "    note: working under the assumption that the rows and columns of\n",
    "          the data corresponds to observations and variables respectively\n",
    "    \"\"\"\n",
    "    def __init__(self, step_size = 0.01, epochs = 10000, random_init = False, \n",
    "                activation_function = \"sigmoid\", batch_size = None):\n",
    "        self.step_size = step_size\n",
    "        self.epochs = epochs\n",
    "        self.random_init = random_init\n",
    "        self.activation_function = activation_function\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        h = 1/(1+np.exp(-z))\n",
    "        return h, z\n",
    "    \n",
    "    def relu(self, z):\n",
    "        h = np.maximum(z, 0)\n",
    "        return h, z\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Using normalization as done here\n",
    "        https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "        \"\"\"\n",
    "        s = np.max(z, axis=1).reshape(z.shape[0], 1)\n",
    "        e_x = np.exp(z - s)\n",
    "        div = np.sum(e_x, axis=1).reshape(z.shape[0], 1)\n",
    "        h = e_x / div \n",
    "        return h, z\n",
    "\n",
    "    def loss(self, hL, y):\n",
    "        return (-1 / y.shape[1]) * np.sum(y * np.log(hL))\n",
    "            \n",
    "    def add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis = 1)\n",
    "    \n",
    "    def undo_one_hot(self, y):\n",
    "        \"\"\"\n",
    "        Undoes one-hot-encoding of target vector\n",
    "        to calculate prediction accuracy\n",
    "        \"\"\"\n",
    "        return np.argmax(y, axis  = 1)\n",
    "    \n",
    "    def one_hot(self, y):\n",
    "        \"\"\"\n",
    "        One-hot encodes a cases x 1 vector of labels\n",
    "        \"\"\"\n",
    "        return np.eye(np.max(y) + 1)[y]\n",
    "        \n",
    "    \n",
    "    def get_preds_and_probs(self, X):\n",
    "        \"\"\"\n",
    "        Returns [predicted probabilties, predictions]\n",
    "        \"\"\"\n",
    "        X = self.add_intercept(X)\n",
    "        \n",
    "        if self.activation_function.lower() == \"softmax\":\n",
    "            probs = self.softmax(np.dot(X, self.w))\n",
    "            preds = np.argmax(probs,axis=1)\n",
    "        else:\n",
    "            probs = self.sigmoid(np.dot(X, self.w))\n",
    "            preds = probs >= .5\n",
    "        \n",
    "        return probs, preds\n",
    "    \n",
    "    def h(self, X):\n",
    "        \"\"\"\n",
    "        Performs activation function check and computes the output based on X\n",
    "        \"\"\"\n",
    "        z = X.dot(self.w)\n",
    "        # activation function\n",
    "        if self.activation_function.lower() == \"sigmoid\":\n",
    "            p = self.sigmoid(z)\n",
    "        else:\n",
    "            p = self.softmax(z)\n",
    "        return p\n",
    "\n",
    "    def initialize_parameters(self, layers_dims, k):\n",
    "        \"\"\"\n",
    "        Returns a dict of initialized parameters\n",
    "        \n",
    "        Arguments\n",
    "        --------\n",
    "        layers_dims: [X.shape[0], # nodes layer 1, ..., # nodes final layer]\n",
    "        k: int. # unique values of y.\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        dict. Final weight matrix is k x nodes in last layer \n",
    "        \n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        parameters = {}\n",
    "        L = len(layers_dims)\n",
    "        \n",
    "        if L == 1:\n",
    "            parameters[\"W\" + str(L)] = np.random.normal(\n",
    "                0, .01, layers_dims[0] * k).reshape(k, layers_dims[0])\n",
    "            parameters[\"b\" + str(L)] = np.zeros((k,1))\n",
    "            return parameters\n",
    "        else:\n",
    "            for l in range(1, L):\n",
    "                parameters[\"W\" + str(l)] = np.random.normal(0, .01, \n",
    "                    layers_dims[l]*layers_dims[l-1]).reshape(layers_dims[l-1],layers_dims[l])\n",
    "                parameters[\"b\" + str(l)] = np.zeros((1,layers_dims[l]))\n",
    "\n",
    "            dim_l = parameters[\"W\" + str(L-1)].shape[0]\n",
    "            parameters[\"W\" + str(L)] = np.random.normal(\n",
    "                0, .01, dim_l * k).reshape(dim_l, k)\n",
    "            parameters[\"b\" + str(L)] = np.zeros((k,1))\n",
    "\n",
    "            return parameters\n",
    "    \n",
    "    def linear_forward(self, h_prev, W, b):\n",
    "        \"\"\"\n",
    "        Computes transformation of the input\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        h_prev: np.array with output values from previous layer\n",
    "        W: np.array with weights, \n",
    "            shape: size of current layer x size of prev layer\n",
    "        b: np.array, \n",
    "            shape: size of current layer x 1\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "        Z: np.array with transformation output\n",
    "        cached_objects: tuple\n",
    "            stores h_prev, W, b for backprop\n",
    "        \"\"\"\n",
    "        Z =  h_prev.dot(W) + b\n",
    "        cache = (h_prev, W, b)\n",
    "        \n",
    "        return Z, cache\n",
    "        \n",
    "    def linear_activation_forward(self, h_prev, W, b, activation_fun):\n",
    "        \"\"\"\n",
    "        Computes output from activation function\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        activation_fun : str\n",
    "            \"sigmoid\", \"relu\" or \"softmax\"\n",
    "        \"\"\"\n",
    "        if activation_fun == \"sigmoid\":\n",
    "            Z, lin_cache = self.linear_forward(h_prev, W, b)\n",
    "            h, activation_cache = self.sigmoid(Z)\n",
    "        \n",
    "        elif activation_fun == \"relu\":\n",
    "            Z, lin_cache = self.linear_forward(h_prev, W, b)\n",
    "            h, activation_cache = self.relu(Z)\n",
    "            \n",
    "        elif activation_fun == \"softmax\":\n",
    "            Z, lin_cache = self.linear_forward(h_prev, W, b)\n",
    "            h, activation_cache = self.softmax(Z)\n",
    "        \n",
    "        cache = (lin_cache, activation_cache)\n",
    "        \n",
    "        return h, cache\n",
    "    \n",
    "    def L_model_forward(self, X, parameters, hidden_layers_acv_fun = \"relu\"):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        parameters : dict\n",
    "            output from self.initialize_parameters\n",
    "        hidden_layers_acv_fun : str or [str]\n",
    "            if str: uses the activation funtcion on all layers\n",
    "            if [str] : allows for different activation functions\n",
    "                       over the layers\n",
    "        \"\"\"\n",
    "        h = X\n",
    "        caches = []\n",
    "        L = len(parameters) // 2 \n",
    "        \n",
    "        if isinstance(hidden_layers_acv_fun, str):\n",
    "            hidden_layers_acv_fun = [hidden_layers_acv_fun] * (L)\n",
    "            \n",
    "        for i in range(1, L):\n",
    "            h_prev = h\n",
    "            h, cache = self.linear_activation_forward(\n",
    "                h_prev, parameters[\"W\" + str(i)], parameters[\"b\" + str(i)],\n",
    "                activation_fun = hidden_layers_acv_fun[i])\n",
    "            caches.append(cache)\n",
    "        \n",
    "        hL, cache = self.linear_activation_forward(\n",
    "            h, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)],\n",
    "            activation_fun = \"softmax\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "        return hL, caches\n",
    "    \n",
    "    def batch_data(self, X, y):\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        # create a permutation of the row ids and \n",
    "        # split them using array_split\n",
    "        ids = np.random.permutation(X.shape[0])\n",
    "        ids_list = np.array_split(ids, np.round(X.shape[0]/self.batch_size))\n",
    "        for batches in ids_list:\n",
    "            X_list.append(X[batches, :])\n",
    "            y_list.append(y[batches, :])\n",
    "        return X_list, y_list\n",
    "    \n",
    "    def sigmoid_grad(self, dh, z):\n",
    "        \"\"\"\n",
    "        computes gradient of the sigmoid function wrt z\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        dh : np.array, post-activation gradient \n",
    "        \"\"\"\n",
    "        h, _ = self.sigmoid(z)\n",
    "        dz = dh * h * (1-h)\n",
    "        \n",
    "        return dz\n",
    "    \n",
    "    def relu_grad(self, dh, z):\n",
    "        \"\"\"\n",
    "        computes gradient of the relu function wrt z\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        dh : np.array, post-activation gradient \n",
    "        \"\"\"\n",
    "        h, _ = self.relu(z)\n",
    "        dz = np.multiply(dh, np.int64(h > 0))\n",
    "        return dz\n",
    "    \n",
    "    def softmax_grad(self, h_prev, z, y):\n",
    "        \"\"\"\n",
    "        Note that I have not made any extension that allows for the \n",
    "        softmax to be the activation of any intermediate layer.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        h_prev: activation output from second to last layer \n",
    "        \"\"\"\n",
    "        h, _ = self.softmax(z)\n",
    "        dz = h_prev * (h - y)\n",
    "        return dz\n",
    "  \n",
    "    def compute_cost(self, hL, y):\n",
    "        return (-1 / y.shape[1]) * np.sum(y * np.log(hL))\n",
    "    \n",
    "    def linear_backward(self, dZ, cache):\n",
    "        \"\"\"\n",
    "        Computes gradient wrt weight, bias and post-activation output of (l-1)\n",
    "        layers at layer l.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        dZ: np.array, gradient of the cost wrt linear output\n",
    "        cache: tuple, values of (h_prev, W, b)\n",
    "        \n",
    "        Returns\n",
    "        dh_prev : np.array, gradient of cost wrt activation (previous l-1 layers)\n",
    "        dW : np.array, gradient of cost wrt W (current layer)\n",
    "        db : np.array, gradient of cost wrt b (current layer)\n",
    "        \"\"\"\n",
    "        h_prev, W, b = cache\n",
    "        m = h_prev.shape[1]\n",
    "        \n",
    "        dW = (1/m) * np.dot(dZ, h_prev.T)\n",
    "        db = (1/m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dh_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "        assert dh_prev.shape == h_prev.shape\n",
    "        assert dW.shape == W.shape\n",
    "        assert db.shape == b.shape\n",
    "        \n",
    "        return dh_prev, dW, db\n",
    "    \n",
    "    def linear_activation_backward(self, dh, cache, activation_fun):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        dh: np.array, post-activation gradient for current layer l\n",
    "        cache: tuple, (lin_cache, activation_cache)\n",
    "        \"\"\"\n",
    "        lin_cache, activation_cache = cache\n",
    "        if activation_fun == \"sigmoid\":\n",
    "            dZ = self.sigmoid_gradient(dh, activation_cache)\n",
    "            dh_prev, dW, db = linear_backward(dZ, lin_cache)\n",
    "        elif activation_fun == \"relu\":\n",
    "            dZ = self.relu_gradient(dh, activation_cache)\n",
    "            dh_prev, dW, db = linear_backward(dZ, lin_cache)\n",
    "        \n",
    "        return dh_prev, dW, db\n",
    "    \n",
    "    def L_model_backward(hL, y, caches, hidden_layers_activation_fn = \"relu\"):\n",
    "        \"\"\"\n",
    "        Computes gradient of output layer wrt weights etc. \n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        AL: np.array, shape = [# nodes in last layer x k]\n",
    "        y: np.array, one-hot encoded class labels\n",
    "        \"\"\"\n",
    "        L = len(caches)\n",
    "        grads = {}\n",
    "        \n",
    "        if isinstance(hidden_layers_acv_fun, str):\n",
    "            hidden_layers_acv_fun = [hidden_layers_acv_fun] * (L)\n",
    "    \n",
    "        dhL = np.divide()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([1,2,-2,\n",
    "              2,3,4, \n",
    "              5,6,2,\n",
    "             -2,-1,2]).reshape(4,3)\n",
    "y = np.array([0,1,0,1,0,0,0,0,1,0,1,0]).reshape(4,3)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,2) and (3,3) not aligned: 2 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-fb129b9ba826>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL_model_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-e8099245f4be>\u001b[0m in \u001b[0;36mL_model_forward\u001b[1;34m(self, X, parameters, hidden_layers_acv_fun)\u001b[0m\n\u001b[0;32m    197\u001b[0m         hL, cache = self.linear_activation_forward(\n\u001b[0;32m    198\u001b[0m             \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             activation_fun = \"softmax\")\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-e8099245f4be>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[1;34m(self, h_prev, W, b, activation_fun)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mactivation_fun\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlin_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-e8099245f4be>\u001b[0m in \u001b[0;36mlinear_forward\u001b[1;34m(self, h_prev, W, b)\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mstores\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \"\"\"\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mh_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,2) and (3,3) not aligned: 2 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "test = neural_net()\n",
    "w = test.initialize_parameters([X.shape[1], 2], y.shape[1])\n",
    "AL, _ = test.L_model_forward(X, w, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = test.sigmoid(X.dot(w[\"W1\"].T) + w[\"b1\"].T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33473931, 0.33321826, 0.33204243],\n",
       "       [0.33469846, 0.33314349, 0.33215805],\n",
       "       [0.33472663, 0.33313986, 0.33213351],\n",
       "       [0.33469606, 0.33319979, 0.33210415]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2 = test.softmax(h1.dot(w[\"W2\"].T) + w[\"b2\"].T)[0]\n",
    "h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24997938, 0.25001267, 0.24995363, 0.25005433],\n",
       "       [0.25004044, 0.24997069, 0.24998366, 0.2500052 ],\n",
       "       [0.24997437, 0.25001626, 0.24995467, 0.2500547 ]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01162813,  0.01845384],\n",
       "       [ 0.05125806, -0.13648573],\n",
       "       [ 0.06683512, -0.14099671],\n",
       "       [-0.00989704, -0.02306596]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dot(w[\"W1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need the following,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta^L = \\nabla_a C \\odot \\sigma'\\big(z^L\\big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^l = \\sigma(w^l a^{l-1} + b^l)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33340699, -0.6665725 ,  0.33316551],\n",
       "       [-0.66675657,  0.33312733,  0.33362925],\n",
       "       [ 0.33335597,  0.33311322, -0.6664692 ],\n",
       "       [ 0.33323417, -0.666647  ,  0.33341283]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = test.softmax(AL.T)[0] - y\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vec = test.undo_one_hot(y)\n",
    "probs[range(4), y_vec] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33340699, -1.6665725 ,  0.33316551],\n",
       "       [-1.66675657,  0.33312733,  0.33362925],\n",
       "       [ 0.33335597,  0.33311322, -1.6664692 ],\n",
       "       [ 0.33323417, -1.666647  ,  0.33341283]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33340699, -1.6665725 ,  0.33316551],\n",
       "       [-1.66675657,  0.33312733,  0.33362925],\n",
       "       [ 0.33335597,  0.33311322, -1.6664692 ],\n",
       "       [ 0.33323417, -1.666647  ,  0.33341283]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net:\n",
    "    \"\"\"\n",
    "    https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Forwad-Back-Propagation.ipynb\n",
    "    \n",
    "    link_function: specifies the link function. Possible values are\n",
    "                   \"sigmoid\" and \"softmax\"\n",
    "    \n",
    "    batch_size: int. Specify the sample size of each bin. Uses np.array_split to avoid\n",
    "                exception due to not even split possible.  Set to None to avoid it.\n",
    "                   \n",
    "    note: working under the assumption that the rows and columns of\n",
    "          the data corresponds to observations and variables respectively\n",
    "    \"\"\"\n",
    "    def __init__(self, step_size = 0.01, epochs = 10000, random_init = False, \n",
    "                activation_function = \"sigmoid\", batch_size = None):\n",
    "        self.step_size = step_size\n",
    "        self.epochs = epochs\n",
    "        self.random_init = random_init\n",
    "        self.activation_function = activation_function\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        h = 1/(1+np.exp(-z))\n",
    "        return h, z\n",
    "    \n",
    "    def relu(self, z):\n",
    "        h = np.maximum(z, 0)\n",
    "        return h, z\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Using normalization as done here\n",
    "        https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "        \"\"\"\n",
    "        s = np.max(z, axis=1).reshape(z.shape[0], 1)\n",
    "        e_x = np.exp(z - s)\n",
    "        div = np.sum(e_x, axis=1).reshape(z.shape[0], 1)\n",
    "        h = e_x / div \n",
    "        return h, z\n",
    "\n",
    "    def loss(self, hL, y):\n",
    "        return (-1 / y.shape[1]) * np.sum(y * np.log(hL))\n",
    "    \n",
    "    def initialize_parameters(self, layers_dims, k):\n",
    "        \"\"\"\n",
    "        Returns a dict of initialized parameters\n",
    "        \n",
    "        Arguments\n",
    "        --------\n",
    "        layers_dims: [X.shape[0], # nodes layer 1, ..., # nodes final layer]\n",
    "        k: int. # unique values of y.\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        dict. Final weight matrix is k x nodes in last layer \n",
    "        \n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        parameters = {}\n",
    "        L = len(layers_dims)\n",
    "        \n",
    "        if L == 1:\n",
    "            parameters[\"W\" + str(L)] = np.random.normal(\n",
    "                0, .01, layers_dims[0] * k).reshape(layers_dims[0],k)\n",
    "            parameters[\"b\" + str(L)] = np.zeros((k,1))\n",
    "            return parameters\n",
    "        else:\n",
    "            for l in range(1, L):\n",
    "                parameters[\"W\" + str(l)] = np.random.normal(0, .01, \n",
    "                    layers_dims[l]*layers_dims[l-1]).reshape(layers_dims[l],layers_dims[l-1])\n",
    "                parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "            dim_l = parameters[\"W\" + str(L-1)].shape[0]\n",
    "            parameters[\"W\" + str(L)] = np.random.normal(\n",
    "                0, .01, dim_l * k).reshape(k, dim_l)\n",
    "            parameters[\"b\" + str(L)] = np.zeros((k, 1))\n",
    "\n",
    "            return parameters\n",
    "    \n",
    "    ## forward methods ##\n",
    "    def linear_forward(self, h_prev, W, b):\n",
    "        \"\"\"\n",
    "        Computes affine transformation of input previous hidden\n",
    "        \"\"\"\n",
    "        Z = h_prev.dot(W.T) + b.T\n",
    "        cache = (h_prev, W, b)\n",
    "        return Z, cache\n",
    "    \n",
    "    def linear_activation_forward(self, h_prev, W, b, activation_fun):\n",
    "        \"\"\"\n",
    "        Computes output from activation function\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        activation_fun : str\n",
    "            \"sigmoid\", \"relu\" or \"softmax\"\n",
    "        \"\"\"\n",
    "        if activation_fun == \"sigmoid\":\n",
    "            Z, lin_cache = self.linear_forward(h_prev, W, b)\n",
    "            h, activation_cache = self.sigmoid(Z)\n",
    "        \n",
    "        elif activation_fun == \"relu\":\n",
    "            Z, lin_cache = self.linear_forward(h_prev, W, b)\n",
    "            h, activation_cache = self.relu(Z)\n",
    "            \n",
    "        elif activation_fun == \"softmax\":\n",
    "            Z, lin_cache = self.linear_forward(h_prev, W, b)\n",
    "            h, activation_cache = self.softmax(Z)\n",
    "        \n",
    "        cache = (lin_cache, activation_cache)\n",
    "        \n",
    "        return h, cache\n",
    "    \n",
    "    def L_model_forward(self, X, parameters, hidden_layers_acv_fun = \"relu\"):\n",
    "        \"\"\"\n",
    "        Goes through the network and returns the final output aswell as storing \n",
    "        information in the cache along the way for backpropagation\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        parameters : dict\n",
    "            output from self.initialize_parameters\n",
    "        hidden_layers_acv_fun : str or [str]\n",
    "            if str: uses the activation funtcion on all layers\n",
    "            if [str] : allows for different activation functions\n",
    "                       over the layers\n",
    "        \"\"\"\n",
    "        h = X\n",
    "        caches = []\n",
    "        L = len(parameters) // 2 \n",
    "        \n",
    "        if isinstance(hidden_layers_acv_fun, str):\n",
    "            hidden_layers_acv_fun = [hidden_layers_acv_fun] * (L)\n",
    "            \n",
    "        for i in range(1, L):\n",
    "            h_prev = h\n",
    "            h, cache = self.linear_activation_forward(\n",
    "                h_prev, parameters[\"W\" + str(i)], parameters[\"b\" + str(i)],\n",
    "                activation_fun = hidden_layers_acv_fun[i-1])\n",
    "            caches.append(cache)\n",
    "        \n",
    "        hL, cache = self.linear_activation_forward(\n",
    "            h, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)],\n",
    "            activation_fun = \"softmax\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "        return hL, caches\n",
    "    \n",
    "    def compute_cost(self, hL, y):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        hL : np.array, output layer\n",
    "        y : np.array, one-hot encoded classes\n",
    "        \"\"\"\n",
    "        return (-1 / y.shape[0]) * np.sum(y * np.log(hL))\n",
    "    \n",
    "    ## Gradients ##\n",
    "    def sigmoid_grad(self, dh, z):\n",
    "        \"\"\"\n",
    "        computes gradient of the sigmoid function wrt z\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        dh : np.array, post-activation gradient \n",
    "        \"\"\"\n",
    "        h, _ = self.sigmoid(z)\n",
    "        dz = dh * h * (1-h)\n",
    "        \n",
    "        return dz\n",
    "    \n",
    "    def relu_grad(self, dh, z):\n",
    "        \"\"\"\n",
    "        computes gradient of the relu function wrt z\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        dh : np.array, post-activation gradient \n",
    "        \"\"\"\n",
    "        h, _ = self.relu(z)\n",
    "        dz = np.multiply(dh, np.int64(h > 0))\n",
    "        \n",
    "        return dz\n",
    "    \n",
    "    def softmax_grad(self, dh, z):\n",
    "        \"\"\"\n",
    "        Computes gradient of the relu function wrt z\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        dh : np.array, gradient of cost-function\n",
    "        \"\"\"\n",
    "        h,_  = self.softmax(z)\n",
    "        dz = dh \n",
    "    \n",
    "    ## Back prop ##\n",
    "    def linear_backward(dZ, cache):\n",
    "        \"\"\"\n",
    "        Computes gradient of output wrt weight, bias and post-act output\n",
    "        of layer (l-1) at layer (l)\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        dZ : np.array, gradient of cost wrt linear output of current layer\n",
    "        cache : tuple, values of (h_prev, W, b) from forwards step at current layer\n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        dh_prev : np.array, gradient of cost wrt activation of previous layer\n",
    "        dW : np.array, gradient of cost wrt W of current layer\n",
    "        db : np.array, gradient of cost wrt b of current layer\n",
    "        \"\"\"\n",
    "        h_prev, W, b = cache\n",
    "        m = h_prev.shape[1]\n",
    "        \n",
    "        dW = 1/m * dZ.dot(h_prev.T)\n",
    "        db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dh_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "        return dh_prev, dW, db\n",
    "\n",
    "    def linear_activation_backward(self, dh, cache, activation_fun):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        dh: np.array, post-activation gradient for current layer l\n",
    "        cache: tuple, (lin_cache, activation_cache)\n",
    "        \"\"\"\n",
    "        lin_cache, activation_cache = cache\n",
    "        \n",
    "        if activation_fun == \"sigmoid\":\n",
    "            dZ = self.sigmoid_gradient(dh, activation_cache)\n",
    "            dh_prev, dW, db = linear_backward(dZ, lin_cache)\n",
    "            \n",
    "        elif activation_fun == \"relu\":\n",
    "            dZ = self.relu_gradient(dh, activation_cache)\n",
    "            dh_prev, dW, db = linear_backward(dZ, lin_cache)\n",
    "        \n",
    "        return dh_prev, dW, db\n",
    "    \n",
    "    def L_model_backward(hL, y, caches, hidden_layers_activation_fn=\"relu\"):\n",
    "        \"\"\"\n",
    "        Computes gradient of output layer wrt weights etc. \n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        AL: np.array, shape = [# nodes in last layer x k]\n",
    "        y: np.array, one-hot encoded class labels\n",
    "        \"\"\"\n",
    "        L = len(caches)\n",
    "        grads = {}\n",
    "        \n",
    "        if isinstance(hidden_layers_acv_fun, str):\n",
    "            hidden_layers_acv_fun = [hidden_layers_acv_fun] * (L)\n",
    "        \n",
    "        # compute gradient of the cost function wrt final hidden layer output \n",
    "        # http://cs231n.github.io/neural-networks-case-study/\n",
    "        dhL = (hL - y) / y.shape[0]\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1,2,-2,2,3,4,5,6,2,-2,-1,2]).reshape(4,3)\n",
    "y = np.array([0,1,0,1,0,0,0,0,1,0,1,0]).reshape(4,3)\n",
    "test = neural_net()\n",
    "w = test.initialize_parameters([X.shape[1], 2, 3], y.shape[1])\n",
    "hL, c = test.L_model_forward(X, w, [\"relu\", \"sigmoid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((array([[ 1,  2, -2],\n",
       "          [ 2,  3,  4],\n",
       "          [ 5,  6,  2],\n",
       "          [-2, -1,  2]]), array([[ 0.01624345, -0.00611756, -0.00528172],\n",
       "          [-0.01072969,  0.00865408, -0.02301539]]), array([[0.],\n",
       "          [0.]])), array([[ 0.01457176,  0.05260924],\n",
       "         [-0.00699266, -0.08755869],\n",
       "         [ 0.03394845, -0.04775475],\n",
       "         [-0.03693278, -0.03322548]])), ((array([[0.01457176, 0.05260924],\n",
       "          [0.        , 0.        ],\n",
       "          [0.03394845, 0.        ],\n",
       "          [0.        , 0.        ]]), array([[ 0.01744812, -0.00761207],\n",
       "          [ 0.00319039, -0.0024937 ],\n",
       "          [ 0.01462108, -0.02060141]]), array([[0.],\n",
       "          [0.],\n",
       "          [0.]])), array([[-1.46215378e-04, -8.47022474e-05, -8.70769511e-04],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 5.92336520e-04,  1.08308823e-04,  4.96362957e-04],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])), ((array([[0.49996345, 0.49997882, 0.49978231],\n",
       "          [0.5       , 0.5       , 0.5       ],\n",
       "          [0.50014808, 0.50002708, 0.50012409],\n",
       "          [0.5       , 0.5       , 0.5       ]]),\n",
       "   array([[-0.00322417, -0.00384054,  0.01133769],\n",
       "          [-0.01099891, -0.00172428, -0.00877858],\n",
       "          [ 0.00042214,  0.00582815, -0.01100619]]),\n",
       "   array([[0.],\n",
       "          [0.],\n",
       "          [0.]])),\n",
       "  array([[ 0.00213422, -0.01074854, -0.00237569],\n",
       "         [ 0.00213649, -0.01075089, -0.00237795],\n",
       "         [ 0.00213731, -0.01075365, -0.0023791 ],\n",
       "         [ 0.00213649, -0.01075089, -0.00237795]]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
