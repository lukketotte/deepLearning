{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_reg:\n",
    "    \"\"\"\n",
    "    link_function: specifies the link function. Possible values are\n",
    "                   \"sigmoid\" and \"softmax\"\n",
    "    \n",
    "    batch_size: int. Specify the sample size of each bin. Uses np.array_split to avoid\n",
    "                exception due to not even split possible.  Set to None to avoid it.\n",
    "                   \n",
    "    note: working under the assumption that the rows and columns of\n",
    "          the data corresponds to observations and variables respectively\n",
    "    \"\"\"\n",
    "    def __init__(self, step_size = 0.01, epochs = 10000, random_init = False, \n",
    "                link_function = \"sigmoid\", batch_size = None):\n",
    "        self.step_size = step_size\n",
    "        self.epochs = epochs\n",
    "        self.random_init = random_init\n",
    "        self.link_function = link_function\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Using normalization as done here\n",
    "        https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "        \"\"\"\n",
    "        s = np.max(z, axis=1).reshape(z.shape[0], 1)\n",
    "        e_x = np.exp(z - s)\n",
    "        div = np.sum(e_x, axis=1).reshape(z.shape[0], 1)\n",
    "        return e_x / div\n",
    "    \n",
    "    def loss(self, p, y):\n",
    "        if np.size(y.shape) == 1:\n",
    "            # y has been supplied as 1d array\n",
    "            return (-y * np.log(p) - (1-y) * np.log(1-p)).mean()\n",
    "        else:\n",
    "            # if y is 2d, the loss function can ge used for K = 2 also\n",
    "            return (-1 / y.shape[0]) * np.sum(y * np.log(p))\n",
    "            \n",
    "    def add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis = 1)\n",
    "    \n",
    "    def undo_one_hot(self, y):\n",
    "        \"\"\"\n",
    "        Undoes one-hot-encoding of target vector\n",
    "        to calculate prediction accuracy\n",
    "        \"\"\"\n",
    "        return np.argmax(y, axis  = 1)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # add an intercept for the b term\n",
    "        X = self.add_intercept(X)\n",
    "        \n",
    "        # initialize the weight matrix\n",
    "        k = y.shape[1]\n",
    "        if self.random_init:\n",
    "            self.w = np.random.normal(0, .01, k * X.shape[1]).reshape(X.shape[1], k)\n",
    "        else:\n",
    "            self.w = np.zeros(k * X.shape[1]).reshape(X.shape[1], k)\n",
    "\n",
    "        #  model fitting\n",
    "        for i in range(self.epochs):\n",
    "            z = np.dot(X, self.w)\n",
    "            \n",
    "            # activation function (?)\n",
    "            if self.link_function.lower() == \"sigmoid\":\n",
    "                p = self.sigmoid(z)\n",
    "            else:\n",
    "                p = self.softmax(z)\n",
    "                \n",
    "            # compute the gradient and update the weight matrix\n",
    "            gradient =  X.T.dot(p - y) / X.shape[0]\n",
    "            self.w -= self.step_size * gradient     \n",
    "            \n",
    "            # update and compute loss\n",
    "            z = np.dot(X, self.w)\n",
    "            # not very efficient\n",
    "            if self.link_function.lower() == \"sigmoid\":\n",
    "                p = self.sigmoid(z)\n",
    "            else:\n",
    "                p = self.softmax(z)\n",
    "            loss = self.loss(p, y)\n",
    "    \n",
    "    def predict(self, X, p_cutoff = .5):\n",
    "        X = self.add_intercept(X)\n",
    "        predicted_prob = self.sigmoid(np.dot(X, self.w))\n",
    "        return predicted_prob >= p_cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log_reg class is what I used in assignment 1a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1\n",
    "We are now looking at the multinomial distribution with $K$ possible outcomes. That is, our data is of the form $\\{\\mathbf{x}_i, y_i\\}_{i=1}^n$ where $y_i \\in \\{1, \\ldots, K\\}$. \n",
    "\n",
    "For this end, I will use the softmax function which is defined as\n",
    "$$\n",
    "p_i^{(k)} = \\frac{\\exp\\left\\{ \\mathbf{w}_{(k)}^T \\mathbf{x}_i + b\\right\\}}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_{(j)}^T \\mathbf{x}_i + b \\right\\}},\n",
    "$$\n",
    "where $p_i^{(k)} = P(\\mathbf{y}_{(i)} = k| \\mathbf{x}_i, \\mathbf{w})$, and $\\mathbf{w}$ is now $p \\times K$. This means that\n",
    "$$\n",
    "p_i = \\frac{1}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_{(j)}^T \\mathbf{x}_i + b \\right\\}} \n",
    "\\begin{bmatrix} \\exp\\{\\mathbf{w}^T_{(1)}\\mathbf{x}_i\\} \\\\ \\vdots \\\\\n",
    "\\exp\\{\\mathbf{w}^T_{(K)}\\mathbf{x}_i\\}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The cross entropy loss, with the softmax activation (correct word?) function is then\n",
    "$$\n",
    "J = - \\frac{1}{n} \\sum_{i=1}^n L_i = - \\frac{1}{n} \\sum_{i=1}^n \\sum_{k = 1}^K y_{ik} \\log p_i^{(k)},\n",
    "$$\n",
    "which simplifies to the cost function of assignment 1a with $K = 2$. It should also be noted that I will treat $\\mathbf{y}$ as a $n\\times K$ matrix where each row has one $1$ and the rest are zeroes, with the position of the $1$ corresponding to class adherence of that observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the derivative of $p_i^{(k)}$, whilst dropping the intercept as it will be accounted for by inserting a column of ones into $\\mathbf{X}$,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial p_i^{(k)}}{\\partial \\mathbf{w}_k} &= \\frac{\\partial}{\\partial \\mathbf{w}_k}\\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} = \\mathbf{x}_i \\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} - \\mathbf{x}_i \\left(\\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}}\\right)^2 = \\mathbf{x}_i\\ p_i^{(k)}\\big(1-p_i^{(k)}\\big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial p_i^{(k)}}{\\partial \\mathbf{w}_l} = \\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\} \\frac{\\partial}{\\partial \\mathbf{w}_l} \\frac{1}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} = \\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\} \\left( - \\mathbf{x}_i\\frac{\\exp\\left\\{ \\mathbf{w}_l^T \\mathbf{x}_i\\right\\}}{\\left(\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}\\right)^2} \\right) = -\\mathbf{x}_i p_i^{(k)}p_i^{(l)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative of $L_i$ wrt to $\\mathbf{w}_l$ gives\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_i}{\\partial\\mathbf{w}_l} &=- \\frac{\\partial}{\\partial \\mathbf{w}_l} \\sum_{k=1}^K y_{ik}\\log p^{(k)}_i = - \\sum_{k=1}^K y_{ik} \\frac{1}{ p^{(k)}_i} \\frac{\\partial}{\\partial \\mathbf{w}_l} p^{(k)}_i = - x_i \\frac{y_{il}}{p_i^{(k)}}p_i^{(k)}(1-p_i^{(l)}) + \\sum_{k \\neq l} x_i \\frac{y_{ik}}{p_i^{(k)}}p_i^{(k)}p_i^{(l)} \\\\\n",
    "&= - x_i y_{il}\\big(1-p_i^{(l)} \\big) + x_i\\sum_{k\\neq l}y_{ik}p_i^{(l)} = x_i \\left( \\sum_{k\\neq l}y_{ik}p_i^{(l)} -  y_{il}\\big(1-p_i^{(l)} \\big) \\right) = x_i \\big(p_i^{(l)} - y_{il} \\big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where the last step follows from the one hot encoding of $\\mathbf{y}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the entropy loss wrt $\\mathbf{w}_l$ is then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}_l} = \\frac{1}{n} \\sum_{i = 1}^n \\frac{\\partial}{\\partial \\mathbf{w}_l} L_i = \\frac{1}{n} \\sum_{i = 1} \\mathbf{x}_i \\big( p_i^{(l)} - y_{il} \\big)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leading to the update step\n",
    "$$\n",
    "\\mathbf{w}_k^{new} = \\mathbf{w}_k^{old} - \\eta \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\\big(p_i^{(l)} - y_{il}\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Using normalization as done here\n",
    "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    \"\"\"\n",
    "    s = np.max(z, axis=1).reshape(z.shape[0], 1)\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1).reshape(z.shape[0], 1)\n",
    "    return e_x / div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on simulated data\n",
    "np.random.seed(123)\n",
    "mean_vec = [1, 5, 3]\n",
    "cov_mat = np.diag([2.2, 15.3, 82.5])\n",
    "n = 100\n",
    "# X is 100 x 3\n",
    "X = np.random.multivariate_normal(mean_vec, cov_mat, n)\n",
    "# coef matrix is 3 x K, so how many K?\n",
    "K = 4\n",
    "# w = np.linspace(1,12, 12).reshape(X.shape[1], K)\n",
    "w = np.random.normal(size = K * X.shape[1]).reshape(X.shape[1], K)\n",
    "# returns n x K, the P matrix\n",
    "p = softmax(X.dot(w))\n",
    "# have to generate data by some for loop as the p values\n",
    "# supplied to np.rand.multinomial has to be 1d\n",
    "y = np.zeros([n, K])\n",
    "for i in range(n):\n",
    "    y[i,:] = np.random.multinomial(1, p[i, :], size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update step, in matrix form is \n",
    "$$\n",
    "\\mathbf{w}^{new} = \\mathbf{w}^{old} - \\eta\\ \\nabla_\\mathbf{w}\\ J,\n",
    "$$\n",
    "where $\\nabla_\\mathbf{w}\\ J$ is,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\mathbf{w}\\ J = \\frac{1}{n}\\; \\underset{p \\times n}{\\mathbf{X}^T}\\left(\\underset{n \\times K}{\\mathbf{p}} - \\underset{n \\times K}{\\mathbf{y}} \\right)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one update step\n",
    "gradient = X.T.dot(p - y) / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train = np.zeros(X.shape[1] * K).reshape(X.shape[1], K)\n",
    "for i in range(1000):\n",
    "    p = softmax(X.dot(w_train))\n",
    "    gradient = X.T.dot(p - y) / X.shape[0]\n",
    "    w_train -= 0.01 * gradient\n",
    "\n",
    "loss = (-1 / X.shape[0]) * np.sum(y * np.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2723480831066777"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbsAndPreds(X, w):\n",
    "    probs = softmax(np.dot(X,w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.89\n"
     ]
    }
   ],
   "source": [
    "_,preds = getProbsAndPreds(X, w_train)\n",
    "y_single = np.argmax(y, axis  = 1)\n",
    "\n",
    "print(\"accuracy = \", np.sum(preds == y_single)/n)\n",
    "# preds, y_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "# want to use array_split as it does not raise exception\n",
    "X_list = np.array_split(X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.125 3.0\n"
     ]
    }
   ],
   "source": [
    "# possible to specify batch sample sizes?\n",
    "batch_size = 32\n",
    "print(X.shape[0]/batch_size, np.round(X.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = np.array_split(X, np.round(X.shape[0]/batch_size))\n",
    "y_list = np.array_split(y, np.round(X.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train = np.zeros(X.shape[1] * K).reshape(X.shape[1], K)\n",
    "for i in range(1000):\n",
    "    gradient = 0\n",
    "    for i in range(len(X_list)):\n",
    "        p = softmax(X_list[i].dot(w_train))\n",
    "        # compute the gradient sequentially\n",
    "        gradient += X_list[i].T.dot(p - y_list[i]) / X_list[i].shape[0]\n",
    "    w_train -= 0.01 * gradient\n",
    "\n",
    "loss = (-1 / X.shape[0]) * np.sum(y * np.log(softmax(X.dot(w_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24836208718023237"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"A\".lower() == \"a\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the log_reg class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = log_reg(step_size = .01, epochs = 10000, link_function = \"softmax\")\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.softmax(np.dot(X, w)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22624009,  1.35058611, -0.76981477, -0.80701143],\n",
       "       [ 0.57622081, -0.59522437, -1.2556304 ,  1.27463396],\n",
       "       [-0.89722548,  1.14645968, -0.95022873,  0.70099454],\n",
       "       [-0.45158178, -0.64663869,  0.91702401,  0.18119646]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9\n"
     ]
    }
   ],
   "source": [
    "X_m = model.add_intercept(X)\n",
    "_,preds = getProbsAndPreds(X_m, model.w)\n",
    "y_single = np.argmax(y, axis  = 1)\n",
    "\n",
    "print(\"accuracy = \", np.sum(preds == y_single)/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is working so far :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
