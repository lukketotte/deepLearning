{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class log_reg:\n",
    "    \"\"\"\n",
    "    link_function: specifies the link function. Possible values are\n",
    "                   \"sigmoid\" and \"softmax\"\n",
    "    \n",
    "    batch_size: int.\n",
    "                specify the number of bins to divide data into\n",
    "                   \n",
    "    note: working under the assumption that the rows and columns of\n",
    "          the data corresponds to observations and variables respectively\n",
    "    \"\"\"\n",
    "    def __init__(self, step_size = 0.01, epochs = 10000, random_init = False, \n",
    "                link_function = \"sigmoid\", batch_size = None):\n",
    "        self.step_size = step_size\n",
    "        self.epochs = epochs\n",
    "        self.random_init = random_init\n",
    "        self.link_function = link_function\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def softmax(z):\n",
    "        \"\"\"\n",
    "        Using normalization as done here\n",
    "        https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "        \"\"\"\n",
    "        s = np.max(z, axis=1).reshape(z.shape[0], 1)\n",
    "        e_x = np.exp(z - s)\n",
    "        div = np.sum(e_x, axis=1).reshape(z.shape[0], 1)\n",
    "        return e_x / div\n",
    "    \n",
    "    def loss(self, h, y):\n",
    "        # h: sigmoid applies to z\n",
    "        # TODO: write it on one form that works for both\n",
    "        # 2 and K-class cases\n",
    "        if np.size(y.shape) == 1:\n",
    "            # y has been supplied as matrix\n",
    "            return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
    "            \n",
    "    \n",
    "    def add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis = 1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # add an intercept for the b term\n",
    "        X = self.add_intercept(X)\n",
    "        # initialize weights dependeing on link function\n",
    "        if not self.random_init and self.link_function.lower() == 'sigmoid':\n",
    "            self.w = np.zeros(X.shape[1]).reshape(X.shape[1],1)\n",
    "        elif self.random_init and self.link_function.lower() == 'sigmoid':\n",
    "            self.w = np.random.rand(X.shape[1]).reshape(X.shape[1],1)\n",
    "        elif self.link_function.lower() == \"softmax\":\n",
    "            k = np.unique(y).size\n",
    "            # generated from N(0, .01)\n",
    "            self.w = np.random.normal(0, .01, k * X.shape[1]).reshape(X.shape[1], k)\n",
    "            \n",
    "        \n",
    "        # time for model fitting\n",
    "        for i in range(self.epochs):\n",
    "            z = np.dot(X, self.w)\n",
    "            h = self.sigmoid(z)\n",
    "            # as calculated in the exercises but not including\n",
    "            # the gradient wrt b as it is accounted for in \n",
    "            # the data matrix by the column of 1s\n",
    "            gradient = np.dot(X.T, (h-y)) / y.size\n",
    "            # update\n",
    "            self.w -= self.step_size * gradient     \n",
    "            \n",
    "            z = np.dot(X, self.w)\n",
    "            h = self.sigmoid(z)\n",
    "            loss = self.loss(h, y)\n",
    "    \n",
    "    def predict(self, X, p_cutoff = .5):\n",
    "        X = self.add_intercept(X)\n",
    "        predicted_prob = self.sigmoid(np.dot(X, self.w))\n",
    "        return predicted_prob >= p_cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log_reg class is what I used in assignment 1a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1\n",
    "We are now looking at the multinomial distribution with $K$ possible outcomes. That is, our data is of the form $\\{\\mathbf{x}_i, y_i\\}_{i=1}^n$ where $y_i \\in \\{1, \\ldots, K\\}$. \n",
    "\n",
    "For this end, I will use the softmax function which is defined as\n",
    "$$\n",
    "p_i^{(k)} = \\frac{\\exp\\left\\{ \\mathbf{w}_{(k)}^T \\mathbf{x}_i + b\\right\\}}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_{(j)}^T \\mathbf{x}_i + b \\right\\}},\n",
    "$$\n",
    "where $p_i^{(k)} = P(\\mathbf{y}_{(i)} = k| \\mathbf{x}_i, \\mathbf{w})$, and $\\mathbf{w}$ is now $p \\times K$. This means that\n",
    "$$\n",
    "p_i = \\frac{1}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_{(j)}^T \\mathbf{x}_i + b \\right\\}} \n",
    "\\begin{bmatrix} \\exp\\{\\mathbf{w}^T_{(1)}\\mathbf{x}_i\\} \\\\ \\vdots \\\\\n",
    "\\exp\\{\\mathbf{w}^T_{(K)}\\mathbf{x}_i\\}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The cross entropy loss, with the softmax activation (correct word?) function is then\n",
    "$$\n",
    "J = - \\frac{1}{n} \\sum_{i=1}^n L_i = - \\frac{1}{n} \\sum_{i=1}^n \\sum_{k = 1}^K y_{ik} \\log p_i^{(k)},\n",
    "$$\n",
    "which simplifies to the cost function of assignment 1a with $K = 2$. It should also be noted that I will treat $\\mathbf{y}$ as a $n\\times K$ matrix where each row has one $1$ and the rest are zeroes, with the position of the $1$ corresponding to class adherence of that observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the derivative of $p_i^{(k)}$, whilst dropping the intercept as it will be accounted for by inserting a column of ones into $\\mathbf{X}$,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial p_i^{(k)}}{\\partial \\mathbf{w}_k} &= \\frac{\\partial}{\\partial \\mathbf{w}_k}\\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} = \\mathbf{x}_i \\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} - \\mathbf{x}_i \\left(\\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}}\\right)^2 = \\mathbf{x}_i\\ p_i^{(k)}\\big(1-p_i^{(k)}\\big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial p_i^{(k)}}{\\partial \\mathbf{w}_l} = \\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\} \\frac{\\partial}{\\partial \\mathbf{w}_l} \\frac{1}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} = \\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\} \\left( - \\mathbf{x}_i\\frac{\\exp\\left\\{ \\mathbf{w}_l^T \\mathbf{x}_i\\right\\}}{\\left(\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}\\right)^2} \\right) = -\\mathbf{x}_i p_i^{(k)}p_i^{(l)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative of $L_i$ wrt to $\\mathbf{w}_l$ gives\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_i}{\\partial\\mathbf{w}_l} &=- \\frac{\\partial}{\\partial \\mathbf{w}_l} \\sum_{k=1}^K y_{ik}\\log p^{(k)}_i = - \\sum_{k=1}^K y_{ik} \\frac{1}{ p^{(k)}_i} \\frac{\\partial}{\\partial \\mathbf{w}_l} p^{(k)}_i = - x_i \\frac{y_{il}}{p_i^{(k)}}p_i^{(k)}(1-p_i^{(l)}) + \\sum_{k \\neq l} x_i \\frac{y_{ik}}{p_i^{(k)}}p_i^{(k)}p_i^{(l)} \\\\\n",
    "&= - x_i y_{il}\\big(1-p_i^{(l)} \\big) + x_i\\sum_{k\\neq l}y_{ik}p_i^{(l)} = x_i \\left( \\sum_{k\\neq l}y_{ik}p_i^{(l)} -  y_{il}\\big(1-p_i^{(l)} \\big) \\right) = x_i \\big(p_i^{(l)} - y_{il} \\big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where the last step follows from the one hot encoding of $\\mathbf{y}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the entropy loss wrt $\\mathbf{w}_l$ is then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}_l} = \\frac{1}{n} \\sum_{i = 1}^n \\frac{\\partial}{\\partial \\mathbf{w}_l} L_i = \\frac{1}{n} \\sum_{i = 1} \\mathbf{x}_i \\big( p_i^{(l)} - y_{il} \\big)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leading to the update step\n",
    "$$\n",
    "\\mathbf{w}_k^{new} = \\mathbf{w}_k^{old} - \\eta \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\\big(p_i^{(l)} - y_{il}\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Using normalization as done here\n",
    "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    \"\"\"\n",
    "    s = np.max(z, axis=1).reshape(z.shape[0], 1)\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1).reshape(z.shape[0], 1)\n",
    "    return e_x / div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on simulated data\n",
    "np.random.seed(123)\n",
    "mean_vec = [1, 5, 3]\n",
    "cov_mat = np.diag([2.2, 15.3, 82.5])\n",
    "n = 30\n",
    "# X is 100 x 3\n",
    "X = np.random.multivariate_normal(mean_vec, cov_mat, n)\n",
    "# coef matrix is 3 x K, so how many K?\n",
    "K = 4\n",
    "w = np.linspace(1,12, 12).reshape(X.shape[1], K)\n",
    "# returns n x K, the P matrix\n",
    "p = softmax(X.dot(w))\n",
    "# have to generate data by some for loop as the p values\n",
    "# supplied to np.rand.multinomial has to be 1d\n",
    "y = np.zeros([n, K])\n",
    "for i in range(n):\n",
    "    y[i,:] = np.random.multinomial(1, p[i, :], size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update step, in matrix form is \n",
    "$$\n",
    "\\mathbf{w}^{new} = \\mathbf{w}^{old} - \\eta\\ \\nabla_\\mathbf{w}\\ J,\n",
    "$$\n",
    "where $\\nabla_\\mathbf{w}\\ J$ is,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\mathbf{w}\\ J = \\frac{1}{n}\\; \\underset{p \\times n}{\\mathbf{X}^T}\\left(\\underset{n \\times K}{\\mathbf{p}} - \\underset{n \\times K}{\\mathbf{y}} \\right)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.00595791e-05, 9.56532517e-04, 3.04380328e-02, 9.68575375e-01],\n",
       "       [9.88839043e-01, 1.10364048e-02, 1.23177004e-04, 1.37477509e-06],\n",
       "       [9.99997351e-01, 2.64874268e-06, 7.01585635e-12, 1.85832474e-17],\n",
       "       [8.12499485e-01, 1.53178021e-01, 2.88781797e-02, 5.44431411e-03],\n",
       "       [5.48154519e-26, 1.44300776e-17, 3.79869418e-09, 9.99999996e-01],\n",
       "       [8.83586461e-23, 1.98380674e-15, 4.45399445e-08, 9.99999955e-01],\n",
       "       [9.97670797e-28, 9.98446594e-19, 9.99222995e-10, 9.99999999e-01],\n",
       "       [1.34790713e-27, 1.22022517e-18, 1.10463803e-09, 9.99999999e-01],\n",
       "       [9.70931054e-01, 2.82246152e-02, 8.20479374e-04, 2.38510392e-05],\n",
       "       [9.96980632e-01, 3.01025137e-03, 9.08905651e-06, 2.74432059e-08],\n",
       "       [9.98988305e-01, 1.01067186e-03, 1.02249206e-06, 1.03445049e-09],\n",
       "       [1.45986825e-08, 5.96835405e-06, 2.44003184e-03, 9.97553985e-01],\n",
       "       [2.70826798e-14, 9.01827364e-10, 3.00299897e-05, 9.99969969e-01],\n",
       "       [2.28778711e-08, 8.05128379e-06, 2.83344418e-03, 9.97158482e-01],\n",
       "       [2.09539775e-11, 7.59983906e-08, 2.75640049e-04, 9.99724284e-01],\n",
       "       [2.65458276e-25, 4.13045136e-17, 6.42685874e-09, 9.99999994e-01],\n",
       "       [6.11383062e-33, 3.34355857e-22, 1.82854001e-11, 1.00000000e+00],\n",
       "       [1.66165111e-07, 3.01689610e-05, 5.47748080e-03, 9.94492184e-01],\n",
       "       [5.64921854e-20, 1.47228604e-13, 3.83703724e-07, 9.99999616e-01],\n",
       "       [6.57512604e-43, 7.56140885e-29, 8.69563618e-15, 1.00000000e+00],\n",
       "       [5.73420558e-08, 1.48509920e-05, 3.84625141e-03, 9.96138840e-01],\n",
       "       [1.49721604e-05, 6.02392437e-04, 2.42367593e-02, 9.75145876e-01],\n",
       "       [1.73298996e-09, 1.44219274e-06, 1.20019155e-03, 9.98798365e-01],\n",
       "       [1.35451081e-15, 1.22420284e-10, 1.10643088e-05, 9.99988936e-01],\n",
       "       [1.64054483e-16, 2.99680126e-11, 5.47428977e-06, 9.99994526e-01],\n",
       "       [1.85822570e-22, 3.25635917e-15, 5.70645160e-08, 9.99999943e-01],\n",
       "       [9.51547510e-01, 4.61098383e-02, 2.23437838e-03, 1.08272918e-04],\n",
       "       [2.51985994e-14, 8.59501602e-10, 2.93168280e-05, 9.99970682e-01],\n",
       "       [5.12120023e-02, 1.14788793e-01, 2.57292557e-01, 5.76706647e-01],\n",
       "       [4.65701686e-09, 2.78716783e-06, 1.66808597e-03, 9.98329122e-01]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform one update step\n",
    "gradient = X.T.dot(p - y) / X.shape[0]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train = np.random.normal(size = K * X.shape[1]).reshape(X.shape[1], K)\n",
    "for i in range(25000):\n",
    "    p = softmax(X.dot(w_train))\n",
    "    gradient = X.T.dot(p - y) / X.shape[0]\n",
    "    w_train -= 0.01 * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.40830145, -1.46253138, -1.33024772,  0.06061024],\n",
       "       [-1.31148047, -0.28176093,  1.02760393,  1.82290059],\n",
       "       [-1.57663284,  0.11901017,  0.72308201,  1.39939365]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.],\n",
       "       [ 5.,  6.,  7.,  8.],\n",
       "       [ 9., 10., 11., 12.]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the math seems to be in order as it is working for this small scale example! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(1, 12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
