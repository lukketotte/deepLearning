{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class log_reg:\n",
    "    \"\"\"\n",
    "    link_function: specifies the link function. Possible values are\n",
    "                   \"sigmoid\" and \"softmax\"\n",
    "                   \n",
    "    note: working under the assumption that the rows and columns of\n",
    "          the data corresponds to observations and variables respectively\n",
    "    \"\"\"\n",
    "    def __init__(self, step_size = 0.01, epochs = 10000, random_init = False, \n",
    "                link_function = \"sigmoid\"):\n",
    "        self.step_size = step_size\n",
    "        self.epochs = epochs\n",
    "        self.random_init = random_init\n",
    "        self.link_function = link_function\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        return: n by K matrix where n is the sample\n",
    "                and K are the possible outcomes of y\n",
    "        \"\"\"\n",
    "        return np.exp(z)/np.sum(np.exp(z), axis = 0)\n",
    "\n",
    "    def loss(self, h, y):\n",
    "        # h: sigmoid applies to z\n",
    "        return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
    "    \n",
    "    def add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis = 1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # add an intercept for the b term\n",
    "        X = self.add_intercept(X)\n",
    "        # initialize weights dependeing on link function\n",
    "        if not self.random_init and self.link_function.lower() == 'sigmoid':\n",
    "            self.w = np.zeros(X.shape[1]).reshape(X.shape[1],1)\n",
    "        elif self.random_init and self.link_function.lower() == 'sigmoid':\n",
    "            self.w = np.random.rand(X.shape[1]).reshape(X.shape[1],1)\n",
    "        elif self.link_function.lower() == \"softmax\":\n",
    "            k = np.unique(y).size\n",
    "            # generated from N(0, .01)\n",
    "            self.w = np.random.normal(0, .01, k * X.shape[1]).reshape(X.shape[1], k)\n",
    "            \n",
    "        \n",
    "        # time for model fitting\n",
    "        for i in range(self.epochs):\n",
    "            z = np.dot(X, self.w)\n",
    "            h = self.sigmoid(z)\n",
    "            # as calculated in the exercises but not including\n",
    "            # the gradient wrt b as it is accounted for in \n",
    "            # the data matrix by the column of 1s\n",
    "            gradient = np.dot(X.T, (h-y)) / y.size\n",
    "            # update\n",
    "            self.w -= self.step_size * gradient     \n",
    "            \n",
    "            z = np.dot(X, self.w)\n",
    "            h = self.sigmoid(z)\n",
    "            loss = self.loss(h, y)\n",
    "    \n",
    "    def predict(self, X, p_cutoff = .5):\n",
    "        X = self.add_intercept(X)\n",
    "        predicted_prob = self.sigmoid(np.dot(X, self.w))\n",
    "        return predicted_prob >= p_cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log_reg class is what I used in assignment 1a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1\n",
    "We are now looking at the multinomial distribution with $K$ possible outcomes. That is, our data is of the form $\\{\\mathbf{x}_i, y_i\\}_{i=1}^n$ where $y_i \\in \\{1, \\ldots, K\\}$. \n",
    "\n",
    "For this end, I will use the softmax function which is defined as\n",
    "$$\n",
    "p_i^{(k)} = \\frac{\\exp\\left\\{ \\mathbf{w}_{(k)}^T \\mathbf{x}_i + b\\right\\}}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_{(j)}^T \\mathbf{x}_i + b \\right\\}},\n",
    "$$\n",
    "where $p_i^{(k)} = P(\\mathbf{y}_{(i)} = k| \\mathbf{x}_i, \\mathbf{w})$, and $\\mathbf{w}$ is now $p \\times K$. This means that\n",
    "$$\n",
    "p_i = \\frac{1}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_{(j)}^T \\mathbf{x}_i + b \\right\\}} \n",
    "\\begin{bmatrix} \\exp\\{\\mathbf{w}^T_{(1)}\\mathbf{x}_i\\} \\\\ \\vdots \\\\\n",
    "\\exp\\{\\mathbf{w}^T_{(K)}\\mathbf{x}_i\\}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The cross entropy loss, with the softmax activation (correct word?) function is then\n",
    "$$\n",
    "J = - \\frac{1}{n} \\sum_{i=1}^n L_i = - \\frac{1}{n} \\sum_{i=1}^n \\sum_{k = 1}^K y_{ik} \\log p_i^{(k)},\n",
    "$$\n",
    "which simplifies to the cost function of assignment 1a with $K = 2$. It should also be noted that I will treat $\\mathbf{y}$ as a $n\\times K$ matrix where each row has one $1$ and the rest are zeroes, with the position of the $1$ corresponding to class adherence of that observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the derivative of $p_i^{(k)}$, whilst dropping the intercept as it will be accounted for by inserting a column of ones into $\\mathbf{X}$,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial p_i^{(k)}}{\\partial \\mathbf{w}_k} &= \\frac{\\partial}{\\partial \\mathbf{w}_k}\\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} = \\mathbf{x}_i \\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} - \\mathbf{x}_i \\left(\\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}}\\right)^2 = \\mathbf{x}_i\\ p_i^{(k)}\\big(1-p_i^{(k)}\\big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial p_i^{(k)}}{\\partial \\mathbf{w}_l} = \\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\} \\frac{\\partial}{\\partial \\mathbf{w}_l} \\frac{1}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} = \\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\} \\left( - \\mathbf{x}_i\\frac{\\exp\\left\\{ \\mathbf{w}_l^T \\mathbf{x}_i\\right\\}}{\\left(\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}\\right)^2} \\right) = -\\mathbf{x}_i p_i^{(k)}p_i^{(l)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative of $L_i$ wrt to $\\mathbf{w}_l$ gives\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_i}{\\partial\\mathbf{w}_l} &=- \\frac{\\partial}{\\partial \\mathbf{w}_l} \\sum_{k=1}^K y_{ik}\\log p^{(k)}_i = - \\sum_{k=1}^K y_{ik} \\frac{1}{ p^{(k)}_i} \\frac{\\partial}{\\partial \\mathbf{w}_l} p^{(k)}_i = - x_i \\frac{y_{ik}}{p_i^{(k)}}p_i^{(k)}(1-p_i^{(l)}) + \\sum_{k \\neq l} x_i \\frac{y_{ik}}{p_i^{(k)}}p_i^{(k)}p_i^{(l)} \\\\\n",
    "&= - x_i y_{ik}\\big(1-p_i^{(l)} \\big) + x_i\\sum_{k\\neq l}y_{ik}p_i^{(l)} = x_i \\left( \\sum_{k\\neq l}y_{ik}p_i^{(l)} -  y_{ik}\\big(1-p_i^{(l)} \\big) \\right) = x_i \\big(p_i^{(l)} - y_{ik} \\big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where the last step follows from the one hot encoding of $\\mathbf{y}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the entropy loss wrt $w_{k}$ is then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial w_{k}} &= -\\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w_k} \\sum_{k=1}^K \\mathbb{1}\\{y_i = k\\}\\log\\left(\\frac{\\exp\\left\\{ \\mathbf{w}_{(k)}^T \\mathbf{x}_i + b\\right\\}}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_{(j)}^T \\mathbf{x}_i + b \\right\\}}\\right) \\\\\n",
    "&= -\\frac{1}{n} \\sum_{i=1}^n \\mathbb{1}\\{y_i = k\\}\\left[ \\frac{\\partial}{\\partial w_k}  \\mathbf{w}_{(k)}^T \\mathbf{x}_i - \\frac{\\partial}{\\partial w_k}\\log \\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_{(j)}^T \\mathbf{x}_i + b \\right\\} \\right] \\\\\n",
    "&= -\\frac{}{}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax test\n",
    "X = np.array([1.,2.,3.,4.,5.,6.]).reshape(2,3)\n",
    "w = np.array([-1., 2., -2., 1., .5, 3.]).reshape(3,2)\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z)/ np.sum(np.exp(z), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99447221e-01, 1.52299795e-08],\n",
       "       [5.52778637e-04, 9.99999985e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(X.dot(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
