{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class log_reg:\n",
    "    \"\"\"\n",
    "    link_function: specifies the link function. Possible values are\n",
    "                   \"sigmoid\" and \"softmax\"\n",
    "    \n",
    "    batch_size: int.\n",
    "                specify the number of bins to divide data into\n",
    "                   \n",
    "    note: working under the assumption that the rows and columns of\n",
    "          the data corresponds to observations and variables respectively\n",
    "    \"\"\"\n",
    "    def __init__(self, step_size = 0.01, epochs = 10000, random_init = False, \n",
    "                link_function = \"sigmoid\", batch_size = None):\n",
    "        self.step_size = step_size\n",
    "        self.epochs = epochs\n",
    "        self.random_init = random_init\n",
    "        self.link_function = link_function\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def softmax(z):\n",
    "        \"\"\"\n",
    "        Using normalization as done here\n",
    "        https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "        \"\"\"\n",
    "        s = np.max(z, axis=1).reshape(z.shape[0], 1)\n",
    "        e_x = np.exp(z - s)\n",
    "        div = np.sum(e_x, axis=1).reshape(z.shape[0], 1)\n",
    "        return e_x / div\n",
    "    \n",
    "    def loss(self, h, y):\n",
    "        # h: sigmoid applies to z\n",
    "        # TODO: write it on one form that works for both\n",
    "        # 2 and K-class cases\n",
    "        if np.size(y.shape) == 1:\n",
    "            # y has been supplied as matrix\n",
    "            return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
    "        else:\n",
    "            \n",
    "    \n",
    "    def add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis = 1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # add an intercept for the b term\n",
    "        X = self.add_intercept(X)\n",
    "        # initialize weights dependeing on link function\n",
    "        if not self.random_init and self.link_function.lower() == 'sigmoid':\n",
    "            self.w = np.zeros(X.shape[1]).reshape(X.shape[1],1)\n",
    "        elif self.random_init and self.link_function.lower() == 'sigmoid':\n",
    "            self.w = np.random.rand(X.shape[1]).reshape(X.shape[1],1)\n",
    "        elif self.link_function.lower() == \"softmax\":\n",
    "            k = np.unique(y).size\n",
    "            # generated from N(0, .01)\n",
    "            self.w = np.random.normal(0, .01, k * X.shape[1]).reshape(X.shape[1], k)\n",
    "            \n",
    "        \n",
    "        # time for model fitting\n",
    "        for i in range(self.epochs):\n",
    "            z = np.dot(X, self.w)\n",
    "            h = self.sigmoid(z)\n",
    "            # as calculated in the exercises but not including\n",
    "            # the gradient wrt b as it is accounted for in \n",
    "            # the data matrix by the column of 1s\n",
    "            gradient = np.dot(X.T, (h-y)) / y.size\n",
    "            # update\n",
    "            self.w -= self.step_size * gradient     \n",
    "            \n",
    "            z = np.dot(X, self.w)\n",
    "            h = self.sigmoid(z)\n",
    "            loss = self.loss(h, y)\n",
    "    \n",
    "    def predict(self, X, p_cutoff = .5):\n",
    "        X = self.add_intercept(X)\n",
    "        predicted_prob = self.sigmoid(np.dot(X, self.w))\n",
    "        return predicted_prob >= p_cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log_reg class is what I used in assignment 1a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1\n",
    "We are now looking at the multinomial distribution with $K$ possible outcomes. That is, our data is of the form $\\{\\mathbf{x}_i, y_i\\}_{i=1}^n$ where $y_i \\in \\{1, \\ldots, K\\}$. \n",
    "\n",
    "For this end, I will use the softmax function which is defined as\n",
    "$$\n",
    "p_i^{(k)} = \\frac{\\exp\\left\\{ \\mathbf{w}_{(k)}^T \\mathbf{x}_i + b\\right\\}}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_{(j)}^T \\mathbf{x}_i + b \\right\\}},\n",
    "$$\n",
    "where $p_i^{(k)} = P(\\mathbf{y}_{(i)} = k| \\mathbf{x}_i, \\mathbf{w})$, and $\\mathbf{w}$ is now $p \\times K$. This means that\n",
    "$$\n",
    "p_i = \\frac{1}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_{(j)}^T \\mathbf{x}_i + b \\right\\}} \n",
    "\\begin{bmatrix} \\exp\\{\\mathbf{w}^T_{(1)}\\mathbf{x}_i\\} \\\\ \\vdots \\\\\n",
    "\\exp\\{\\mathbf{w}^T_{(K)}\\mathbf{x}_i\\}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The cross entropy loss, with the softmax activation (correct word?) function is then\n",
    "$$\n",
    "J = - \\frac{1}{n} \\sum_{i=1}^n L_i = - \\frac{1}{n} \\sum_{i=1}^n \\sum_{k = 1}^K y_{ik} \\log p_i^{(k)},\n",
    "$$\n",
    "which simplifies to the cost function of assignment 1a with $K = 2$. It should also be noted that I will treat $\\mathbf{y}$ as a $n\\times K$ matrix where each row has one $1$ and the rest are zeroes, with the position of the $1$ corresponding to class adherence of that observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the derivative of $p_i^{(k)}$, whilst dropping the intercept as it will be accounted for by inserting a column of ones into $\\mathbf{X}$,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial p_i^{(k)}}{\\partial \\mathbf{w}_k} &= \\frac{\\partial}{\\partial \\mathbf{w}_k}\\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K\\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} = \\mathbf{x}_i \\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} - \\mathbf{x}_i \\left(\\frac{\\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\}}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}}\\right)^2 = \\mathbf{x}_i\\ p_i^{(k)}\\big(1-p_i^{(k)}\\big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial p_i^{(k)}}{\\partial \\mathbf{w}_l} = \\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\} \\frac{\\partial}{\\partial \\mathbf{w}_l} \\frac{1}{\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}} = \\exp\\left\\{ \\mathbf{w}_k^T \\mathbf{x}_i\\right\\} \\left( - \\mathbf{x}_i\\frac{\\exp\\left\\{ \\mathbf{w}_l^T \\mathbf{x}_i\\right\\}}{\\left(\\sum_{j=1}^K \\exp\\left\\{ \\mathbf{w}_j^T \\mathbf{x}_i\\right\\}\\right)^2} \\right) = -\\mathbf{x}_i p_i^{(k)}p_i^{(l)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative of $L_i$ wrt to $\\mathbf{w}_l$ gives\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_i}{\\partial\\mathbf{w}_l} &=- \\frac{\\partial}{\\partial \\mathbf{w}_l} \\sum_{k=1}^K y_{ik}\\log p^{(k)}_i = - \\sum_{k=1}^K y_{ik} \\frac{1}{ p^{(k)}_i} \\frac{\\partial}{\\partial \\mathbf{w}_l} p^{(k)}_i = - x_i \\frac{y_{il}}{p_i^{(k)}}p_i^{(k)}(1-p_i^{(l)}) + \\sum_{k \\neq l} x_i \\frac{y_{ik}}{p_i^{(k)}}p_i^{(k)}p_i^{(l)} \\\\\n",
    "&= - x_i y_{il}\\big(1-p_i^{(l)} \\big) + x_i\\sum_{k\\neq l}y_{ik}p_i^{(l)} = x_i \\left( \\sum_{k\\neq l}y_{ik}p_i^{(l)} -  y_{il}\\big(1-p_i^{(l)} \\big) \\right) = x_i \\big(p_i^{(l)} - y_{il} \\big),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where the last step follows from the one hot encoding of $\\mathbf{y}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the entropy loss wrt $\\mathbf{w}_l$ is then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}_l} = \\frac{1}{n} \\sum_{i = 1}^n \\frac{\\partial}{\\partial \\mathbf{w}_l} L_i = \\frac{1}{n} \\sum_{i = 1} \\mathbf{x}_i \\big( p_i^{(l)} - y_{il} \\big)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leading to the update step\n",
    "$$\n",
    "\\mathbf{w}_k^{new} = \\mathbf{w}_k^{old} - \\eta \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\\big(p_i^{(l)} - y_{il}\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Using normalization as done here\n",
    "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    \"\"\"\n",
    "    s = np.max(z, axis=1).reshape(z.shape[0], 1)\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1).reshape(z.shape[0], 1)\n",
    "    return e_x / div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on simulated data\n",
    "np.random.seed(12)\n",
    "mean_vec = [1, 5, 3]\n",
    "cov_mat = np.diag([2.2, 15.3, 82.5])\n",
    "n = 30\n",
    "# X is 100 x 3\n",
    "X = np.random.multivariate_normal(mean_vec, cov_mat, n)\n",
    "# coef matrix is 3 x K, so how many K?\n",
    "K = 4\n",
    "w = np.random.uniform(size = K * X.shape[1]).reshape(X.shape[1], K)\n",
    "# returns n x K, the P matrix\n",
    "p = softmax(X.dot(w))\n",
    "# have to generate data by some for loop as the p values\n",
    "# supplied to np.rand.multinomial has to be 1d\n",
    "y = np.zeros([n, K])\n",
    "for i in range(n):\n",
    "    y[i,:] = np.random.multinomial(1, p[i, :], size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update step, in matrix form is \n",
    "$$\n",
    "\\mathbf{w}^{new} = \\mathbf{w}^{old} - \\eta\\ \\nabla_\\mathbf{w}\\ J,\n",
    "$$\n",
    "where $\\nabla_\\mathbf{w}\\ J$ is,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\mathbf{w}\\ J = \\frac{1}{n}\\; \\underset{p \\times n}{\\mathbf{X}^T}\\left(\\underset{n \\times K}{\\mathbf{p}} - \\underset{n \\times K}{\\mathbf{y}} \\right)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one update step\n",
    "gradient = X.T.dot(p - y) / X.shape[0]\n",
    "w_new = w - 0.1 * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train = w\n",
    "for i in range(100):\n",
    "    w_train -= 0.01 * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5.87575378,   30.16754979,  -28.18417633,   -6.22417467],\n",
       "       [  18.03483825,  -45.0854149 ,   -5.98884794,   34.81097869],\n",
       "       [ -34.93176669, -102.54890765,  146.85283132,   -7.72830352]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5.84761496,   30.0201985 ,  -28.04075543,   -6.19210545],\n",
       "       [  17.94609888,  -44.85761247,   -5.9585745 ,   34.64164219],\n",
       "       [ -34.75740091, -102.03813816,  146.12910183,   -7.68970932]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the math seems to be in order as it is working for this small scale example! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
